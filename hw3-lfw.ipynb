{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriculation number: A0153347A\n",
    "Email: e0025553@nus.edu.sg\n",
    "\n",
    "Matriculation number: A0097689Y\n",
    "Email: a0097689@u.nus.edu.sg\n",
    "\n",
    "Matriculation number: A0153196B\n",
    "Email: e0025302@nus.edu.sg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Notes about this assignment \n",
    "\n",
    "Data:\n",
    "X_train.npy: training data numpy array. Each row in the array corresponds to an image unrolled to a vector (50 x 37 = 1850 dimension)\n",
    "\n",
    "y_train.npy: labels (0-6) of each data corresponding to the image in the same row in X_train.npy\n",
    "\n",
    "X_test.npy: testing data numpy array for evaluation of your models. The prediction outputs on this test data is to be submitted for scoreboard\n",
    "\n",
    "Ideas:\n",
    "- multiclass SVM (see Multiclass classification)\n",
    "- kNN with PCA \n",
    "- Dense (fully connected neural network)\n",
    "- CNN (convolutional neural network)\n",
    "\n",
    "# While fitting the model, if it terminates too early with a high loss or low f1 score, please restart the kernel and clear all output. It may happen a few times because Adam optimization is stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files included in this submission\n",
    "\n",
    "- hw3-lfw.ipynb\n",
    "- readme.pdf\n",
    "- X_train.npy\n",
    "- Y_train.npy\n",
    "- X_test.npy\n",
    "- model.h5\n",
    "- corrected.lfw.heldout.npy\n",
    "- submission_heldout_testing.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up libraries and packages\n",
    "\n",
    "Setting up Keras, Theano, Lasagne for Mac OS-X users, assuming you have conda and anaconda set up already.\n",
    "\n",
    "1. conda update conda\n",
    "2. Install lasagne: pip install lasagne\n",
    "3. Install the latest version of Theano, pip install git+git://github.com/Theano/Theano.git\n",
    "4. Install the latest version of keras, pip install git+git://github.com/fchollet/keras.git\n",
    "\n",
    "If the above doesn't work, its probably because you have multiple installations of python on your machine, it is installing into the global site-packages of your other python executable. To overcome this, create a conda environment before installing packages into it, run these steps before running the steps above:\n",
    "\n",
    "1. Create a new conda env - conda create --name hw3\n",
    "2. source activate hw3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "\n",
    "#train data\n",
    "\n",
    "xin= np.load(\"./X_train.npy\")\n",
    "yin= np.load(\"./y_train.npy\")\n",
    "xout= np.load(\"./X_test.npy\")\n",
    "enc = OneHotEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def createSubmission(filename, y):\n",
    "    fo = open( filename , 'w' )\n",
    "    fo.write(\"ImageId,PredictedClass\\n\")\n",
    "    for i in range(y.shape[0]):\n",
    "        fo.write(str(i)+\",\"+str(y[i])+\"\\n\")\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If model already exists, load from file and skip the next cell!\n",
    "from keras.models import load_model\n",
    "model = load_model('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 774 samples, validate on 192 samples\n",
      "Epoch 1/400\n",
      "774/774 [==============================] - 10s - loss: 10.1684 - fbeta_score: 0.3168 - val_loss: 8.9797 - val_fbeta_score: 0.4115\n",
      "Epoch 2/400\n",
      "774/774 [==============================] - 10s - loss: 9.8202 - fbeta_score: nan - val_loss: 8.9562 - val_fbeta_score: 0.4115\n",
      "Epoch 3/400\n",
      "774/774 [==============================] - 9s - loss: 9.4800 - fbeta_score: 0.3966 - val_loss: 9.4862 - val_fbeta_score: 0.4115\n",
      "Epoch 4/400\n",
      "774/774 [==============================] - 9s - loss: 9.4496 - fbeta_score: 0.4109 - val_loss: 9.4862 - val_fbeta_score: 0.4115\n",
      "Epoch 5/400\n",
      "774/774 [==============================] - 9s - loss: 9.4835 - fbeta_score: 0.4031 - val_loss: 9.4862 - val_fbeta_score: 0.4115\n",
      "Epoch 6/400\n",
      "774/774 [==============================] - 10s - loss: 9.2381 - fbeta_score: 0.4018 - val_loss: 9.2414 - val_fbeta_score: 0.4115\n",
      "Epoch 7/400\n",
      "774/774 [==============================] - 9s - loss: 9.8874 - fbeta_score: 0.3411 - val_loss: 9.4017 - val_fbeta_score: 0.4115\n",
      "Epoch 8/400\n",
      "774/774 [==============================] - 10s - loss: 9.3314 - fbeta_score: 0.3915 - val_loss: 9.3924 - val_fbeta_score: 0.4115\n",
      "Epoch 9/400\n",
      "774/774 [==============================] - 10s - loss: 9.2458 - fbeta_score: 0.4018 - val_loss: 9.4247 - val_fbeta_score: 0.4115\n",
      "Epoch 10/400\n",
      "774/774 [==============================] - 10s - loss: 9.1097 - fbeta_score: 0.3656 - val_loss: 7.5684 - val_fbeta_score: 0.4115\n",
      "Epoch 11/400\n",
      "774/774 [==============================] - 10s - loss: 8.7168 - fbeta_score: 0.3635 - val_loss: 6.5380 - val_fbeta_score: 0.4167\n",
      "Epoch 12/400\n",
      "774/774 [==============================] - 10s - loss: 7.3999 - fbeta_score: 0.3458 - val_loss: 2.2434 - val_fbeta_score: 0.1936\n",
      "Epoch 13/400\n",
      "774/774 [==============================] - 9s - loss: 4.0509 - fbeta_score: 0.2304 - val_loss: 1.7858 - val_fbeta_score: nan\n",
      "Epoch 14/400\n",
      "774/774 [==============================] - 10s - loss: 2.1676 - fbeta_score: nan - val_loss: 1.8466 - val_fbeta_score: nan\n",
      "Epoch 15/400\n",
      "774/774 [==============================] - 10s - loss: 1.8888 - fbeta_score: 0.1547 - val_loss: 1.8220 - val_fbeta_score: nan\n",
      "Epoch 16/400\n",
      "774/774 [==============================] - 11s - loss: 1.9393 - fbeta_score: nan - val_loss: 1.8104 - val_fbeta_score: nan\n",
      "Epoch 17/400\n",
      "774/774 [==============================] - 11s - loss: 1.7888 - fbeta_score: 0.1608 - val_loss: 1.7953 - val_fbeta_score: nan\n",
      "Epoch 18/400\n",
      "774/774 [==============================] - 11s - loss: 1.7822 - fbeta_score: nan - val_loss: 1.7848 - val_fbeta_score: nan\n",
      "Epoch 19/400\n",
      "774/774 [==============================] - 10s - loss: 1.7804 - fbeta_score: 0.1078 - val_loss: 1.7720 - val_fbeta_score: nan\n",
      "Epoch 20/400\n",
      "774/774 [==============================] - 10s - loss: 1.7261 - fbeta_score: 0.1858 - val_loss: 1.7402 - val_fbeta_score: nan\n",
      "Epoch 21/400\n",
      "774/774 [==============================] - 11s - loss: 1.7032 - fbeta_score: 0.2098 - val_loss: 1.7578 - val_fbeta_score: nan\n",
      "Epoch 22/400\n",
      "774/774 [==============================] - 11s - loss: 1.7037 - fbeta_score: 0.1798 - val_loss: 1.7972 - val_fbeta_score: nan\n",
      "Epoch 23/400\n",
      "774/774 [==============================] - 12s - loss: 1.7409 - fbeta_score: nan - val_loss: 1.7871 - val_fbeta_score: nan\n",
      "Epoch 24/400\n",
      "774/774 [==============================] - 10s - loss: 1.7248 - fbeta_score: nan - val_loss: 1.7650 - val_fbeta_score: nan\n",
      "Epoch 25/400\n",
      "774/774 [==============================] - 10s - loss: 1.7102 - fbeta_score: 0.1568 - val_loss: 1.7510 - val_fbeta_score: nan\n",
      "Epoch 26/400\n",
      "774/774 [==============================] - 10s - loss: 1.6831 - fbeta_score: nan - val_loss: 1.7283 - val_fbeta_score: nan\n",
      "Epoch 27/400\n",
      "774/774 [==============================] - 11s - loss: 1.7058 - fbeta_score: nan - val_loss: 1.7271 - val_fbeta_score: nan\n",
      "Epoch 28/400\n",
      "774/774 [==============================] - 11s - loss: 1.6724 - fbeta_score: nan - val_loss: 1.7375 - val_fbeta_score: nan\n",
      "Epoch 29/400\n",
      "774/774 [==============================] - 12s - loss: 1.6719 - fbeta_score: 0.1999 - val_loss: 1.7030 - val_fbeta_score: nan\n",
      "Epoch 30/400\n",
      "774/774 [==============================] - 10s - loss: 1.6485 - fbeta_score: nan - val_loss: 1.7087 - val_fbeta_score: nan\n",
      "Epoch 31/400\n",
      "774/774 [==============================] - 10s - loss: 1.6746 - fbeta_score: nan - val_loss: 1.6711 - val_fbeta_score: nan\n",
      "Epoch 32/400\n",
      "774/774 [==============================] - 12s - loss: 1.6415 - fbeta_score: nan - val_loss: 1.6714 - val_fbeta_score: nan\n",
      "Epoch 33/400\n",
      "774/774 [==============================] - 12s - loss: 1.6161 - fbeta_score: 0.2172 - val_loss: 1.6700 - val_fbeta_score: nan\n",
      "Epoch 34/400\n",
      "774/774 [==============================] - 11s - loss: 1.5845 - fbeta_score: 0.2499 - val_loss: 1.6190 - val_fbeta_score: 0.0600\n",
      "Epoch 35/400\n",
      "774/774 [==============================] - 11s - loss: 1.5845 - fbeta_score: nan - val_loss: 1.6018 - val_fbeta_score: 0.0788\n",
      "Epoch 36/400\n",
      "774/774 [==============================] - 10s - loss: 1.5667 - fbeta_score: nan - val_loss: 1.6020 - val_fbeta_score: 0.0507\n",
      "Epoch 37/400\n",
      "774/774 [==============================] - 11s - loss: 1.5098 - fbeta_score: 0.3079 - val_loss: 1.5324 - val_fbeta_score: 0.1289\n",
      "Epoch 38/400\n",
      "774/774 [==============================] - 11s - loss: 1.5085 - fbeta_score: 0.3786 - val_loss: 1.5423 - val_fbeta_score: 0.1054\n",
      "Epoch 39/400\n",
      "774/774 [==============================] - 11s - loss: 1.4785 - fbeta_score: 0.3336 - val_loss: 1.5048 - val_fbeta_score: 0.1474\n",
      "Epoch 40/400\n",
      "774/774 [==============================] - 11s - loss: 1.4905 - fbeta_score: 0.3464 - val_loss: 1.4571 - val_fbeta_score: 0.2051\n",
      "Epoch 41/400\n",
      "774/774 [==============================] - 11s - loss: 1.4702 - fbeta_score: 0.3845 - val_loss: 1.4346 - val_fbeta_score: 0.2314\n",
      "Epoch 42/400\n",
      "774/774 [==============================] - 9s - loss: 1.4384 - fbeta_score: 0.3914 - val_loss: 1.3844 - val_fbeta_score: 0.3220\n",
      "Epoch 43/400\n",
      "774/774 [==============================] - 9s - loss: 1.3791 - fbeta_score: nan - val_loss: 1.3700 - val_fbeta_score: 0.2951\n",
      "Epoch 44/400\n",
      "774/774 [==============================] - 9s - loss: 1.3753 - fbeta_score: 0.4172 - val_loss: 1.3350 - val_fbeta_score: 0.3357\n",
      "Epoch 45/400\n",
      "774/774 [==============================] - 9s - loss: 1.3406 - fbeta_score: 0.4536 - val_loss: 1.3331 - val_fbeta_score: 0.3168\n",
      "Epoch 46/400\n",
      "774/774 [==============================] - 9s - loss: 1.2918 - fbeta_score: 0.4644 - val_loss: 1.2467 - val_fbeta_score: 0.4090\n",
      "Epoch 47/400\n",
      "774/774 [==============================] - 9s - loss: 1.2625 - fbeta_score: 0.4553 - val_loss: 1.2415 - val_fbeta_score: 0.3785\n",
      "Epoch 48/400\n",
      "774/774 [==============================] - 9s - loss: 1.2884 - fbeta_score: nan - val_loss: 1.2062 - val_fbeta_score: 0.4526\n",
      "Epoch 49/400\n",
      "774/774 [==============================] - 9s - loss: 1.2739 - fbeta_score: 0.4824 - val_loss: 1.1803 - val_fbeta_score: 0.4754\n",
      "Epoch 50/400\n",
      "774/774 [==============================] - 9s - loss: 1.2262 - fbeta_score: 0.4846 - val_loss: 1.1389 - val_fbeta_score: 0.5183\n",
      "Epoch 51/400\n",
      "774/774 [==============================] - 10s - loss: 1.1736 - fbeta_score: 0.5279 - val_loss: 1.1072 - val_fbeta_score: 0.5449\n",
      "Epoch 52/400\n",
      "774/774 [==============================] - 9s - loss: 1.1382 - fbeta_score: 0.5515 - val_loss: 1.0879 - val_fbeta_score: 0.5558\n",
      "Epoch 53/400\n",
      "774/774 [==============================] - 9s - loss: 1.1245 - fbeta_score: 0.5428 - val_loss: 1.0523 - val_fbeta_score: 0.5592\n",
      "Epoch 54/400\n",
      "774/774 [==============================] - 9s - loss: 1.0866 - fbeta_score: 0.5600 - val_loss: 1.0297 - val_fbeta_score: 0.5746\n",
      "Epoch 55/400\n",
      "774/774 [==============================] - 9s - loss: 1.1094 - fbeta_score: 0.5691 - val_loss: 1.0401 - val_fbeta_score: 0.5826\n",
      "Epoch 56/400\n",
      "774/774 [==============================] - 9s - loss: 1.0449 - fbeta_score: 0.5769 - val_loss: 0.9720 - val_fbeta_score: 0.6381\n",
      "Epoch 57/400\n",
      "774/774 [==============================] - 9s - loss: 1.0481 - fbeta_score: 0.5681 - val_loss: 0.9664 - val_fbeta_score: 0.6036\n",
      "Epoch 58/400\n",
      "774/774 [==============================] - 9s - loss: 1.0247 - fbeta_score: 0.6090 - val_loss: 0.8957 - val_fbeta_score: 0.6382\n",
      "Epoch 59/400\n",
      "774/774 [==============================] - 9s - loss: 0.9637 - fbeta_score: 0.6192 - val_loss: 0.8621 - val_fbeta_score: 0.6497\n",
      "Epoch 60/400\n",
      "774/774 [==============================] - 9s - loss: 0.9570 - fbeta_score: 0.6271 - val_loss: 0.8759 - val_fbeta_score: 0.6473\n",
      "Epoch 61/400\n",
      "774/774 [==============================] - 9s - loss: 0.9223 - fbeta_score: 0.6322 - val_loss: 0.8596 - val_fbeta_score: 0.6486\n",
      "Epoch 62/400\n",
      "774/774 [==============================] - 9s - loss: 0.9167 - fbeta_score: 0.6558 - val_loss: 0.8037 - val_fbeta_score: 0.6856\n",
      "Epoch 63/400\n",
      "774/774 [==============================] - 10s - loss: 0.8771 - fbeta_score: 0.6428 - val_loss: 0.7595 - val_fbeta_score: 0.6770\n",
      "Epoch 64/400\n",
      "774/774 [==============================] - 9s - loss: 0.8259 - fbeta_score: 0.6868 - val_loss: 0.7656 - val_fbeta_score: 0.6744\n",
      "Epoch 65/400\n",
      "774/774 [==============================] - 9s - loss: 0.8287 - fbeta_score: 0.6954 - val_loss: 0.7220 - val_fbeta_score: 0.7155\n",
      "Epoch 66/400\n",
      "774/774 [==============================] - 9s - loss: 0.7776 - fbeta_score: 0.7089 - val_loss: 0.7360 - val_fbeta_score: 0.6927\n",
      "Epoch 67/400\n",
      "774/774 [==============================] - 9s - loss: 0.8009 - fbeta_score: 0.7029 - val_loss: 0.6708 - val_fbeta_score: 0.7430\n",
      "Epoch 68/400\n",
      "774/774 [==============================] - 9s - loss: 0.7543 - fbeta_score: 0.7126 - val_loss: 0.6656 - val_fbeta_score: 0.7565\n",
      "Epoch 69/400\n",
      "774/774 [==============================] - 9s - loss: 0.7007 - fbeta_score: 0.7407 - val_loss: 0.6570 - val_fbeta_score: 0.7619\n",
      "Epoch 70/400\n",
      "774/774 [==============================] - 10s - loss: 0.6975 - fbeta_score: 0.7376 - val_loss: 0.6440 - val_fbeta_score: 0.7574\n",
      "Epoch 71/400\n",
      "774/774 [==============================] - 10s - loss: 0.7372 - fbeta_score: 0.7299 - val_loss: 0.6706 - val_fbeta_score: 0.7442\n",
      "Epoch 72/400\n",
      "774/774 [==============================] - 10s - loss: 0.6601 - fbeta_score: 0.7562 - val_loss: 0.6115 - val_fbeta_score: 0.7838\n",
      "Epoch 73/400\n",
      "774/774 [==============================] - 10s - loss: 0.6759 - fbeta_score: 0.7555 - val_loss: 0.6058 - val_fbeta_score: 0.7927\n",
      "Epoch 74/400\n",
      "774/774 [==============================] - 10s - loss: 0.6348 - fbeta_score: 0.7789 - val_loss: 0.5923 - val_fbeta_score: 0.8037\n",
      "Epoch 75/400\n",
      "774/774 [==============================] - 10s - loss: 0.6957 - fbeta_score: 0.7493 - val_loss: 0.6027 - val_fbeta_score: 0.7873\n",
      "Epoch 76/400\n",
      "774/774 [==============================] - 10s - loss: 0.6296 - fbeta_score: 0.7619 - val_loss: 0.5942 - val_fbeta_score: 0.7786\n",
      "Epoch 77/400\n",
      "774/774 [==============================] - 10s - loss: 0.6812 - fbeta_score: 0.7565 - val_loss: 0.6384 - val_fbeta_score: 0.7677\n",
      "Epoch 78/400\n",
      "774/774 [==============================] - 10s - loss: 0.6177 - fbeta_score: 0.7733 - val_loss: 0.5562 - val_fbeta_score: 0.8011\n",
      "Epoch 79/400\n",
      "774/774 [==============================] - 10s - loss: 0.5828 - fbeta_score: 0.7918 - val_loss: 0.5715 - val_fbeta_score: 0.8155\n",
      "Epoch 80/400\n",
      "774/774 [==============================] - 10s - loss: 0.5498 - fbeta_score: 0.7950 - val_loss: 0.5614 - val_fbeta_score: 0.8316\n",
      "Epoch 81/400\n",
      "774/774 [==============================] - 11s - loss: 0.5396 - fbeta_score: 0.8054 - val_loss: 0.5301 - val_fbeta_score: 0.8169\n",
      "Epoch 82/400\n",
      "774/774 [==============================] - 11s - loss: 0.5346 - fbeta_score: 0.8079 - val_loss: 0.4881 - val_fbeta_score: 0.8505\n",
      "Epoch 83/400\n",
      "774/774 [==============================] - 11s - loss: 0.4983 - fbeta_score: 0.8088 - val_loss: 0.5020 - val_fbeta_score: 0.8327\n",
      "Epoch 84/400\n",
      "774/774 [==============================] - 11s - loss: 0.5062 - fbeta_score: 0.8159 - val_loss: 0.4966 - val_fbeta_score: 0.8419\n",
      "Epoch 85/400\n",
      "774/774 [==============================] - 11s - loss: 0.5203 - fbeta_score: 0.8108 - val_loss: 0.4883 - val_fbeta_score: 0.8490\n",
      "Epoch 86/400\n",
      "774/774 [==============================] - 11s - loss: 0.4838 - fbeta_score: 0.8268 - val_loss: 0.4745 - val_fbeta_score: 0.8418\n",
      "Epoch 87/400\n",
      "774/774 [==============================] - 11s - loss: 0.5015 - fbeta_score: 0.8133 - val_loss: 0.4830 - val_fbeta_score: 0.8585\n",
      "Epoch 88/400\n",
      "774/774 [==============================] - 11s - loss: 0.4722 - fbeta_score: 0.8217 - val_loss: 0.4792 - val_fbeta_score: 0.8342\n",
      "Epoch 89/400\n",
      "774/774 [==============================] - 11s - loss: 0.4630 - fbeta_score: 0.8367 - val_loss: 0.4976 - val_fbeta_score: 0.8544\n",
      "Epoch 90/400\n",
      "774/774 [==============================] - 12s - loss: 0.4313 - fbeta_score: 0.8429 - val_loss: 0.4644 - val_fbeta_score: 0.8406\n",
      "Epoch 91/400\n",
      "774/774 [==============================] - 11s - loss: 0.4520 - fbeta_score: 0.8288 - val_loss: 0.4582 - val_fbeta_score: 0.8545\n",
      "Epoch 92/400\n",
      "774/774 [==============================] - 11s - loss: 0.4034 - fbeta_score: 0.8603 - val_loss: 0.4433 - val_fbeta_score: 0.8534\n",
      "Epoch 93/400\n",
      "774/774 [==============================] - 12s - loss: 0.3836 - fbeta_score: 0.8684 - val_loss: 0.4291 - val_fbeta_score: 0.8704\n",
      "Epoch 94/400\n",
      "774/774 [==============================] - 11s - loss: 0.4105 - fbeta_score: 0.8535 - val_loss: 0.4392 - val_fbeta_score: 0.8660\n",
      "Epoch 95/400\n",
      "774/774 [==============================] - 11s - loss: 0.3836 - fbeta_score: 0.8660 - val_loss: 0.4407 - val_fbeta_score: 0.8610\n",
      "Epoch 96/400\n",
      "774/774 [==============================] - 11s - loss: 0.3969 - fbeta_score: 0.8574 - val_loss: 0.4265 - val_fbeta_score: 0.8730\n",
      "Epoch 97/400\n",
      "774/774 [==============================] - 11s - loss: 0.3683 - fbeta_score: 0.8722 - val_loss: 0.4768 - val_fbeta_score: 0.8352\n",
      "Epoch 98/400\n",
      "774/774 [==============================] - 14s - loss: 0.4161 - fbeta_score: 0.8548 - val_loss: 0.5046 - val_fbeta_score: 0.8278\n",
      "Epoch 99/400\n",
      "774/774 [==============================] - 13s - loss: 0.3722 - fbeta_score: 0.8658 - val_loss: 0.4309 - val_fbeta_score: 0.8618\n",
      "Epoch 100/400\n",
      "774/774 [==============================] - 16s - loss: 0.4057 - fbeta_score: 0.8631 - val_loss: 0.4112 - val_fbeta_score: 0.8819\n",
      "Epoch 101/400\n",
      "774/774 [==============================] - 14s - loss: 0.3242 - fbeta_score: 0.8820 - val_loss: 0.4250 - val_fbeta_score: 0.8698\n",
      "Epoch 102/400\n",
      "774/774 [==============================] - 12s - loss: 0.2849 - fbeta_score: 0.9022 - val_loss: 0.4049 - val_fbeta_score: 0.8907\n",
      "Epoch 103/400\n",
      "774/774 [==============================] - 13s - loss: 0.3513 - fbeta_score: 0.8790 - val_loss: 0.4093 - val_fbeta_score: 0.8759\n",
      "Epoch 104/400\n",
      "774/774 [==============================] - 18s - loss: 0.3190 - fbeta_score: 0.8895 - val_loss: 0.4314 - val_fbeta_score: 0.8547\n",
      "Epoch 105/400\n",
      "774/774 [==============================] - 14s - loss: 0.3256 - fbeta_score: 0.8897 - val_loss: 0.4066 - val_fbeta_score: 0.8800\n",
      "Epoch 106/400\n",
      "774/774 [==============================] - 15s - loss: 0.3213 - fbeta_score: 0.8900 - val_loss: 0.3995 - val_fbeta_score: 0.8829\n",
      "Epoch 107/400\n",
      "774/774 [==============================] - 13s - loss: 0.3033 - fbeta_score: 0.8950 - val_loss: 0.4057 - val_fbeta_score: 0.8665\n",
      "Epoch 108/400\n",
      "774/774 [==============================] - 16s - loss: 0.2592 - fbeta_score: 0.9135 - val_loss: 0.3865 - val_fbeta_score: 0.8617\n",
      "Epoch 109/400\n",
      "774/774 [==============================] - 15s - loss: 0.2851 - fbeta_score: 0.9058 - val_loss: 0.3976 - val_fbeta_score: 0.8756\n",
      "Epoch 110/400\n",
      "774/774 [==============================] - 14s - loss: 0.2905 - fbeta_score: 0.9011 - val_loss: 0.3826 - val_fbeta_score: 0.8796\n",
      "Epoch 111/400\n",
      "774/774 [==============================] - 14s - loss: 0.3054 - fbeta_score: 0.8997 - val_loss: 0.3882 - val_fbeta_score: 0.8876\n",
      "Epoch 112/400\n",
      "774/774 [==============================] - 14s - loss: 0.3150 - fbeta_score: 0.8944 - val_loss: 0.3909 - val_fbeta_score: 0.8811\n",
      "Epoch 113/400\n",
      "774/774 [==============================] - 14s - loss: 0.3068 - fbeta_score: 0.9087 - val_loss: 0.3792 - val_fbeta_score: 0.8915\n",
      "Epoch 114/400\n",
      "774/774 [==============================] - 14s - loss: 0.2783 - fbeta_score: 0.9071 - val_loss: 0.3651 - val_fbeta_score: 0.8764\n",
      "Epoch 115/400\n",
      "774/774 [==============================] - 16s - loss: 0.2605 - fbeta_score: 0.9100 - val_loss: 0.3739 - val_fbeta_score: 0.8847\n",
      "Epoch 116/400\n",
      "774/774 [==============================] - 16s - loss: 0.2160 - fbeta_score: 0.9301 - val_loss: 0.3510 - val_fbeta_score: 0.8950\n",
      "Epoch 117/400\n",
      "774/774 [==============================] - 14s - loss: 0.2646 - fbeta_score: 0.9063 - val_loss: 0.3580 - val_fbeta_score: 0.8950\n",
      "Epoch 118/400\n",
      "774/774 [==============================] - 15s - loss: 0.2359 - fbeta_score: 0.9330 - val_loss: 0.3636 - val_fbeta_score: 0.8860\n",
      "Epoch 119/400\n",
      "774/774 [==============================] - 15s - loss: 0.2354 - fbeta_score: 0.9236 - val_loss: 0.3651 - val_fbeta_score: 0.8937\n",
      "Epoch 120/400\n",
      "774/774 [==============================] - 16s - loss: 0.2588 - fbeta_score: 0.9139 - val_loss: 0.3657 - val_fbeta_score: 0.8903\n",
      "Epoch 121/400\n",
      "774/774 [==============================] - 15s - loss: 0.2530 - fbeta_score: 0.9023 - val_loss: 0.3724 - val_fbeta_score: 0.8852\n",
      "Epoch 122/400\n",
      "774/774 [==============================] - 15s - loss: 0.2407 - fbeta_score: 0.9257 - val_loss: 0.3700 - val_fbeta_score: 0.8851\n",
      "Epoch 123/400\n",
      "774/774 [==============================] - 15s - loss: 0.2279 - fbeta_score: 0.9280 - val_loss: 0.3792 - val_fbeta_score: 0.8914\n",
      "Epoch 124/400\n",
      "774/774 [==============================] - 16s - loss: 0.2180 - fbeta_score: 0.9303 - val_loss: 0.3839 - val_fbeta_score: 0.8744\n",
      "Epoch 125/400\n",
      "774/774 [==============================] - 18s - loss: 0.2459 - fbeta_score: 0.9172 - val_loss: 0.3728 - val_fbeta_score: 0.9005\n",
      "Epoch 126/400\n",
      "774/774 [==============================] - 17s - loss: 0.2209 - fbeta_score: 0.9225 - val_loss: 0.3503 - val_fbeta_score: 0.8999\n",
      "Epoch 127/400\n",
      "774/774 [==============================] - 18s - loss: 0.2235 - fbeta_score: 0.9234 - val_loss: 0.3549 - val_fbeta_score: 0.8883\n",
      "Epoch 128/400\n",
      "774/774 [==============================] - 17s - loss: 0.2028 - fbeta_score: 0.9302 - val_loss: 0.3578 - val_fbeta_score: 0.8920\n",
      "Epoch 129/400\n",
      "774/774 [==============================] - 14s - loss: 0.2065 - fbeta_score: 0.9284 - val_loss: 0.3254 - val_fbeta_score: 0.8983\n",
      "Epoch 130/400\n",
      "774/774 [==============================] - 14s - loss: 0.2232 - fbeta_score: 0.9111 - val_loss: 0.3402 - val_fbeta_score: 0.9012\n",
      "Epoch 131/400\n",
      "774/774 [==============================] - 18s - loss: 0.2122 - fbeta_score: 0.9289 - val_loss: 0.3551 - val_fbeta_score: 0.8925\n",
      "Epoch 132/400\n",
      "774/774 [==============================] - 15s - loss: 0.1959 - fbeta_score: 0.9406 - val_loss: 0.3392 - val_fbeta_score: 0.9012\n",
      "Epoch 133/400\n",
      "774/774 [==============================] - 16s - loss: 0.1952 - fbeta_score: 0.9391 - val_loss: 0.3545 - val_fbeta_score: 0.8979\n",
      "Epoch 134/400\n",
      "774/774 [==============================] - 15s - loss: 0.2134 - fbeta_score: 0.9288 - val_loss: 0.3331 - val_fbeta_score: 0.8957\n",
      "Epoch 135/400\n",
      "774/774 [==============================] - 16s - loss: 0.2228 - fbeta_score: 0.9202 - val_loss: 0.3621 - val_fbeta_score: 0.8959\n",
      "Epoch 136/400\n",
      "774/774 [==============================] - 15s - loss: 0.1900 - fbeta_score: 0.9354 - val_loss: 0.3545 - val_fbeta_score: 0.8790\n",
      "Epoch 137/400\n",
      "774/774 [==============================] - 16s - loss: 0.1908 - fbeta_score: 0.9356 - val_loss: 0.3245 - val_fbeta_score: 0.9019\n",
      "Epoch 138/400\n",
      "774/774 [==============================] - 16s - loss: 0.1638 - fbeta_score: 0.9481 - val_loss: 0.3261 - val_fbeta_score: 0.8982\n",
      "Epoch 139/400\n",
      "774/774 [==============================] - 15s - loss: 0.1502 - fbeta_score: 0.9449 - val_loss: 0.3178 - val_fbeta_score: 0.8990\n",
      "Epoch 140/400\n",
      "774/774 [==============================] - 16s - loss: 0.1743 - fbeta_score: 0.9376 - val_loss: 0.3167 - val_fbeta_score: 0.8961\n",
      "Epoch 141/400\n",
      "774/774 [==============================] - 16s - loss: 0.1757 - fbeta_score: 0.9355 - val_loss: 0.3241 - val_fbeta_score: 0.8932\n",
      "Epoch 142/400\n",
      "774/774 [==============================] - 18s - loss: 0.1529 - fbeta_score: 0.9451 - val_loss: 0.3337 - val_fbeta_score: 0.9045\n",
      "Epoch 143/400\n",
      "774/774 [==============================] - 16s - loss: 0.1615 - fbeta_score: 0.9366 - val_loss: 0.3244 - val_fbeta_score: 0.9018\n",
      "Epoch 144/400\n",
      "774/774 [==============================] - 15s - loss: 0.1690 - fbeta_score: 0.9392 - val_loss: 0.3137 - val_fbeta_score: 0.9071\n",
      "Epoch 145/400\n",
      "774/774 [==============================] - 15s - loss: 0.1393 - fbeta_score: 0.9442 - val_loss: 0.3257 - val_fbeta_score: 0.9067\n",
      "Epoch 146/400\n",
      "774/774 [==============================] - 14s - loss: 0.1518 - fbeta_score: 0.9498 - val_loss: 0.3349 - val_fbeta_score: 0.9064\n",
      "Epoch 147/400\n",
      "774/774 [==============================] - 14s - loss: 0.1846 - fbeta_score: 0.9360 - val_loss: 0.3112 - val_fbeta_score: 0.9155\n",
      "Epoch 148/400\n",
      "774/774 [==============================] - 14s - loss: 0.1516 - fbeta_score: 0.9431 - val_loss: 0.3216 - val_fbeta_score: 0.9154\n",
      "Epoch 149/400\n",
      "774/774 [==============================] - 15s - loss: 0.1448 - fbeta_score: 0.9566 - val_loss: 0.3450 - val_fbeta_score: 0.9077\n",
      "Epoch 150/400\n",
      "774/774 [==============================] - 15s - loss: 0.1540 - fbeta_score: 0.9545 - val_loss: 0.3314 - val_fbeta_score: 0.9067\n",
      "Epoch 151/400\n",
      "774/774 [==============================] - 15s - loss: 0.1413 - fbeta_score: 0.9507 - val_loss: 0.3393 - val_fbeta_score: 0.9076\n",
      "Epoch 152/400\n",
      "774/774 [==============================] - 15s - loss: 0.1635 - fbeta_score: 0.9446 - val_loss: 0.3247 - val_fbeta_score: 0.9097\n",
      "Epoch 153/400\n",
      "774/774 [==============================] - 15s - loss: 0.1250 - fbeta_score: 0.9569 - val_loss: 0.3177 - val_fbeta_score: 0.9073\n",
      "Epoch 154/400\n",
      "774/774 [==============================] - 18s - loss: 0.1371 - fbeta_score: 0.9522 - val_loss: 0.3317 - val_fbeta_score: 0.9101\n",
      "Epoch 155/400\n",
      "774/774 [==============================] - 16s - loss: 0.1541 - fbeta_score: 0.9517 - val_loss: 0.3311 - val_fbeta_score: 0.9148\n",
      "Epoch 156/400\n",
      "774/774 [==============================] - 15s - loss: 0.1264 - fbeta_score: 0.9587 - val_loss: 0.3119 - val_fbeta_score: 0.9153\n",
      "Epoch 157/400\n",
      "774/774 [==============================] - 15s - loss: 0.1599 - fbeta_score: 0.9504 - val_loss: 0.3479 - val_fbeta_score: 0.9015\n",
      "Epoch 158/400\n",
      "774/774 [==============================] - 16s - loss: 0.1395 - fbeta_score: 0.9527 - val_loss: 0.3338 - val_fbeta_score: 0.9099\n",
      "Epoch 159/400\n",
      "774/774 [==============================] - 17s - loss: 0.1041 - fbeta_score: 0.9616 - val_loss: 0.3406 - val_fbeta_score: 0.9104\n",
      "Epoch 160/400\n",
      "774/774 [==============================] - 19s - loss: 0.1275 - fbeta_score: 0.9473 - val_loss: 0.3213 - val_fbeta_score: 0.9077\n",
      "Epoch 161/400\n",
      "774/774 [==============================] - 17s - loss: 0.0993 - fbeta_score: 0.9702 - val_loss: 0.3091 - val_fbeta_score: 0.9083\n",
      "Epoch 162/400\n",
      "774/774 [==============================] - 16s - loss: 0.1374 - fbeta_score: 0.9505 - val_loss: 0.3194 - val_fbeta_score: 0.9095\n",
      "Epoch 163/400\n",
      "774/774 [==============================] - 16s - loss: 0.1325 - fbeta_score: 0.9587 - val_loss: 0.3251 - val_fbeta_score: 0.8984\n",
      "Epoch 164/400\n",
      "774/774 [==============================] - 24s - loss: 0.1206 - fbeta_score: 0.9602 - val_loss: 0.3107 - val_fbeta_score: 0.9000\n",
      "Epoch 165/400\n",
      "774/774 [==============================] - 21s - loss: 0.1155 - fbeta_score: 0.9570 - val_loss: 0.3308 - val_fbeta_score: 0.9071\n",
      "Epoch 166/400\n",
      "774/774 [==============================] - 21s - loss: 0.1210 - fbeta_score: 0.9629 - val_loss: 0.3370 - val_fbeta_score: 0.9135\n",
      "Epoch 167/400\n",
      "774/774 [==============================] - 21s - loss: 0.0875 - fbeta_score: 0.9696 - val_loss: 0.3467 - val_fbeta_score: 0.9048\n",
      "Epoch 168/400\n",
      "774/774 [==============================] - 20s - loss: 0.0986 - fbeta_score: 0.9665 - val_loss: 0.3784 - val_fbeta_score: 0.9025\n",
      "Epoch 169/400\n",
      "774/774 [==============================] - 19s - loss: 0.1548 - fbeta_score: 0.9417 - val_loss: 0.3737 - val_fbeta_score: 0.8988\n",
      "Epoch 170/400\n",
      "774/774 [==============================] - 14s - loss: 0.1263 - fbeta_score: 0.9490 - val_loss: 0.3388 - val_fbeta_score: 0.9207\n",
      "Epoch 171/400\n",
      "774/774 [==============================] - 15s - loss: 0.1007 - fbeta_score: 0.9675 - val_loss: 0.3436 - val_fbeta_score: 0.8992\n",
      "Epoch 172/400\n",
      "774/774 [==============================] - 15s - loss: 0.1346 - fbeta_score: 0.9515 - val_loss: 0.3384 - val_fbeta_score: 0.9046\n",
      "Epoch 173/400\n",
      "774/774 [==============================] - 14s - loss: 0.1214 - fbeta_score: 0.9656 - val_loss: 0.3490 - val_fbeta_score: 0.8984\n",
      "Epoch 174/400\n",
      "774/774 [==============================] - 14s - loss: 0.1235 - fbeta_score: 0.9570 - val_loss: 0.3221 - val_fbeta_score: 0.9081\n",
      "Epoch 175/400\n",
      "774/774 [==============================] - 15s - loss: 0.1126 - fbeta_score: 0.9643 - val_loss: 0.3206 - val_fbeta_score: 0.9024\n",
      "Epoch 176/400\n",
      "774/774 [==============================] - 17s - loss: 0.0906 - fbeta_score: 0.9700 - val_loss: 0.2973 - val_fbeta_score: 0.9163\n",
      "Epoch 177/400\n",
      "774/774 [==============================] - 16s - loss: 0.0951 - fbeta_score: 0.9676 - val_loss: 0.2963 - val_fbeta_score: 0.9186\n",
      "Epoch 178/400\n",
      "774/774 [==============================] - 16s - loss: 0.0785 - fbeta_score: 0.9760 - val_loss: 0.3210 - val_fbeta_score: 0.9162\n",
      "Epoch 179/400\n",
      "774/774 [==============================] - 16s - loss: 0.0977 - fbeta_score: 0.9681 - val_loss: 0.3138 - val_fbeta_score: 0.9243\n",
      "Epoch 180/400\n",
      "774/774 [==============================] - 19s - loss: 0.1176 - fbeta_score: 0.9649 - val_loss: 0.3231 - val_fbeta_score: 0.9212\n",
      "Epoch 181/400\n",
      "774/774 [==============================] - 31s - loss: 0.1249 - fbeta_score: 0.9478 - val_loss: 0.3398 - val_fbeta_score: 0.9076\n",
      "Epoch 182/400\n",
      "774/774 [==============================] - 27s - loss: 0.1352 - fbeta_score: 0.9543 - val_loss: 0.3376 - val_fbeta_score: 0.9044\n",
      "Epoch 183/400\n",
      "774/774 [==============================] - 21s - loss: 0.1327 - fbeta_score: 0.9565 - val_loss: 0.3178 - val_fbeta_score: 0.8996\n",
      "Epoch 184/400\n",
      "774/774 [==============================] - 21s - loss: 0.0868 - fbeta_score: 0.9721 - val_loss: 0.3030 - val_fbeta_score: 0.9159\n",
      "Epoch 185/400\n",
      "774/774 [==============================] - 16s - loss: 0.0991 - fbeta_score: 0.9662 - val_loss: 0.3412 - val_fbeta_score: 0.9163\n",
      "Epoch 186/400\n",
      "774/774 [==============================] - 15s - loss: 0.1045 - fbeta_score: 0.9662 - val_loss: 0.3267 - val_fbeta_score: 0.9268\n",
      "Epoch 187/400\n",
      "774/774 [==============================] - 15s - loss: 0.1073 - fbeta_score: 0.9623 - val_loss: 0.3164 - val_fbeta_score: 0.9163\n",
      "Epoch 188/400\n",
      "774/774 [==============================] - 15s - loss: 0.0965 - fbeta_score: 0.9745 - val_loss: 0.3072 - val_fbeta_score: 0.9186\n",
      "Epoch 189/400\n",
      "774/774 [==============================] - 14s - loss: 0.1004 - fbeta_score: 0.9656 - val_loss: 0.3051 - val_fbeta_score: 0.9215\n",
      "Epoch 190/400\n",
      "774/774 [==============================] - 15s - loss: 0.0945 - fbeta_score: 0.9727 - val_loss: 0.2940 - val_fbeta_score: 0.9215\n",
      "Epoch 191/400\n",
      "774/774 [==============================] - 15s - loss: 0.0937 - fbeta_score: 0.9728 - val_loss: 0.3176 - val_fbeta_score: 0.9131\n",
      "Epoch 192/400\n",
      "774/774 [==============================] - 18s - loss: 0.1033 - fbeta_score: 0.9721 - val_loss: 0.3090 - val_fbeta_score: 0.9267\n",
      "Epoch 193/400\n",
      "774/774 [==============================] - 23s - loss: 0.0774 - fbeta_score: 0.9753 - val_loss: 0.2876 - val_fbeta_score: 0.9239\n",
      "Epoch 194/400\n",
      "774/774 [==============================] - 22s - loss: 0.0702 - fbeta_score: 0.9760 - val_loss: 0.2852 - val_fbeta_score: 0.9239\n",
      "Epoch 195/400\n",
      "774/774 [==============================] - 24s - loss: 0.0812 - fbeta_score: 0.9734 - val_loss: 0.2862 - val_fbeta_score: 0.9167\n",
      "Epoch 196/400\n",
      "774/774 [==============================] - 24s - loss: 0.1069 - fbeta_score: 0.9530 - val_loss: 0.2879 - val_fbeta_score: 0.9160\n",
      "Epoch 197/400\n",
      "774/774 [==============================] - 21s - loss: 0.0826 - fbeta_score: 0.9733 - val_loss: 0.2996 - val_fbeta_score: 0.9135\n",
      "Epoch 198/400\n",
      "774/774 [==============================] - 21s - loss: 0.0842 - fbeta_score: 0.9714 - val_loss: 0.3262 - val_fbeta_score: 0.9138\n",
      "Epoch 199/400\n",
      "774/774 [==============================] - 18s - loss: 0.0850 - fbeta_score: 0.9715 - val_loss: 0.3293 - val_fbeta_score: 0.9167\n",
      "Epoch 200/400\n",
      "774/774 [==============================] - 15s - loss: 0.0692 - fbeta_score: 0.9799 - val_loss: 0.3084 - val_fbeta_score: 0.9243\n",
      "Epoch 201/400\n",
      "774/774 [==============================] - 15s - loss: 0.0553 - fbeta_score: 0.9767 - val_loss: 0.3052 - val_fbeta_score: 0.9139\n",
      "Epoch 202/400\n",
      "774/774 [==============================] - 15s - loss: 0.0654 - fbeta_score: 0.9766 - val_loss: 0.3126 - val_fbeta_score: 0.9139\n",
      "Epoch 203/400\n",
      "774/774 [==============================] - 15s - loss: 0.0661 - fbeta_score: 0.9753 - val_loss: 0.3108 - val_fbeta_score: 0.9138\n",
      "Epoch 204/400\n",
      "774/774 [==============================] - 14s - loss: 0.0611 - fbeta_score: 0.9812 - val_loss: 0.2926 - val_fbeta_score: 0.9110\n",
      "Epoch 205/400\n",
      "774/774 [==============================] - 15s - loss: 0.0628 - fbeta_score: 0.9837 - val_loss: 0.2961 - val_fbeta_score: 0.9110\n",
      "Epoch 206/400\n",
      "774/774 [==============================] - 15s - loss: 0.0655 - fbeta_score: 0.9786 - val_loss: 0.2980 - val_fbeta_score: 0.9163\n",
      "Epoch 207/400\n",
      "774/774 [==============================] - 16s - loss: 0.0712 - fbeta_score: 0.9760 - val_loss: 0.2921 - val_fbeta_score: 0.9163\n",
      "Epoch 208/400\n",
      "774/774 [==============================] - 14s - loss: 0.0783 - fbeta_score: 0.9740 - val_loss: 0.3028 - val_fbeta_score: 0.9188\n",
      "Epoch 209/400\n",
      "774/774 [==============================] - 14s - loss: 0.0718 - fbeta_score: 0.9760 - val_loss: 0.2933 - val_fbeta_score: 0.9161\n",
      "Epoch 210/400\n",
      "774/774 [==============================] - 15s - loss: 0.0806 - fbeta_score: 0.9721 - val_loss: 0.2958 - val_fbeta_score: 0.9215\n",
      "Epoch 211/400\n",
      "774/774 [==============================] - 15s - loss: 0.0751 - fbeta_score: 0.9740 - val_loss: 0.3112 - val_fbeta_score: 0.9167\n",
      "Epoch 212/400\n",
      "774/774 [==============================] - 16s - loss: 0.0513 - fbeta_score: 0.9858 - val_loss: 0.3163 - val_fbeta_score: 0.9185\n",
      "Epoch 213/400\n",
      "774/774 [==============================] - 14s - loss: 0.0550 - fbeta_score: 0.9819 - val_loss: 0.3323 - val_fbeta_score: 0.9086\n",
      "Epoch 214/400\n",
      "774/774 [==============================] - 16s - loss: 0.0786 - fbeta_score: 0.9727 - val_loss: 0.2881 - val_fbeta_score: 0.9317\n",
      "Epoch 215/400\n",
      "774/774 [==============================] - 14s - loss: 0.0620 - fbeta_score: 0.9851 - val_loss: 0.2738 - val_fbeta_score: 0.9242\n",
      "Epoch 216/400\n",
      "774/774 [==============================] - 14s - loss: 0.0552 - fbeta_score: 0.9831 - val_loss: 0.2864 - val_fbeta_score: 0.9243\n",
      "Epoch 217/400\n",
      "774/774 [==============================] - 15s - loss: 0.0458 - fbeta_score: 0.9864 - val_loss: 0.2706 - val_fbeta_score: 0.9240\n",
      "Epoch 218/400\n",
      "774/774 [==============================] - 16s - loss: 0.0600 - fbeta_score: 0.9754 - val_loss: 0.2639 - val_fbeta_score: 0.9267\n",
      "Epoch 219/400\n",
      "774/774 [==============================] - 23s - loss: 0.0768 - fbeta_score: 0.9715 - val_loss: 0.2821 - val_fbeta_score: 0.9295\n",
      "Epoch 220/400\n",
      "774/774 [==============================] - 16s - loss: 0.0642 - fbeta_score: 0.9714 - val_loss: 0.3127 - val_fbeta_score: 0.9296\n",
      "Epoch 221/400\n",
      "774/774 [==============================] - 16s - loss: 0.0657 - fbeta_score: 0.9792 - val_loss: 0.2803 - val_fbeta_score: 0.9240\n",
      "Epoch 222/400\n",
      "774/774 [==============================] - 16s - loss: 0.0695 - fbeta_score: 0.9818 - val_loss: 0.3094 - val_fbeta_score: 0.9115\n",
      "Epoch 223/400\n",
      "774/774 [==============================] - 16s - loss: 0.0499 - fbeta_score: 0.9870 - val_loss: 0.3089 - val_fbeta_score: 0.9294\n",
      "Epoch 224/400\n",
      "774/774 [==============================] - 16s - loss: 0.0535 - fbeta_score: 0.9825 - val_loss: 0.2891 - val_fbeta_score: 0.9210\n",
      "Epoch 225/400\n",
      "774/774 [==============================] - 17s - loss: 0.0508 - fbeta_score: 0.9792 - val_loss: 0.3056 - val_fbeta_score: 0.9269\n",
      "Epoch 226/400\n",
      "774/774 [==============================] - 19s - loss: 0.0579 - fbeta_score: 0.9812 - val_loss: 0.3065 - val_fbeta_score: 0.9240\n",
      "Epoch 227/400\n",
      "774/774 [==============================] - 15s - loss: 0.0552 - fbeta_score: 0.9780 - val_loss: 0.2841 - val_fbeta_score: 0.9237\n",
      "Epoch 228/400\n",
      "774/774 [==============================] - 15s - loss: 0.0896 - fbeta_score: 0.9656 - val_loss: 0.3082 - val_fbeta_score: 0.9135\n",
      "Epoch 229/400\n",
      "774/774 [==============================] - 15s - loss: 0.0673 - fbeta_score: 0.9792 - val_loss: 0.2918 - val_fbeta_score: 0.9268\n",
      "Epoch 230/400\n",
      "774/774 [==============================] - 15s - loss: 0.0449 - fbeta_score: 0.9858 - val_loss: 0.2861 - val_fbeta_score: 0.9219\n",
      "Epoch 231/400\n",
      "774/774 [==============================] - 16s - loss: 0.0593 - fbeta_score: 0.9792 - val_loss: 0.2925 - val_fbeta_score: 0.9320\n",
      "Epoch 232/400\n",
      "774/774 [==============================] - 17s - loss: 0.0532 - fbeta_score: 0.9825 - val_loss: 0.3001 - val_fbeta_score: 0.9243\n",
      "Epoch 233/400\n",
      "774/774 [==============================] - 16s - loss: 0.0581 - fbeta_score: 0.9793 - val_loss: 0.3072 - val_fbeta_score: 0.9243\n",
      "Epoch 234/400\n",
      "774/774 [==============================] - 15s - loss: 0.0561 - fbeta_score: 0.9819 - val_loss: 0.3084 - val_fbeta_score: 0.9219\n",
      "Epoch 235/400\n",
      "774/774 [==============================] - 15s - loss: 0.0529 - fbeta_score: 0.9851 - val_loss: 0.3135 - val_fbeta_score: 0.9271\n",
      "Epoch 236/400\n",
      "774/774 [==============================] - 15s - loss: 0.0549 - fbeta_score: 0.9780 - val_loss: 0.2878 - val_fbeta_score: 0.9271\n",
      "Epoch 237/400\n",
      "774/774 [==============================] - 15s - loss: 0.0747 - fbeta_score: 0.9701 - val_loss: 0.2777 - val_fbeta_score: 0.9238\n",
      "Epoch 238/400\n",
      "774/774 [==============================] - 15s - loss: 0.0658 - fbeta_score: 0.9716 - val_loss: 0.2892 - val_fbeta_score: 0.9159\n",
      "Epoch 239/400\n",
      "774/774 [==============================] - 15s - loss: 0.0480 - fbeta_score: 0.9883 - val_loss: 0.2722 - val_fbeta_score: 0.9187\n",
      "Epoch 240/400\n",
      "774/774 [==============================] - 15s - loss: 0.0577 - fbeta_score: 0.9774 - val_loss: 0.2847 - val_fbeta_score: 0.9239\n",
      "Epoch 241/400\n",
      "774/774 [==============================] - 15s - loss: 0.0301 - fbeta_score: 0.9942 - val_loss: 0.2967 - val_fbeta_score: 0.9162\n",
      "Epoch 242/400\n",
      "774/774 [==============================] - 15s - loss: 0.0522 - fbeta_score: 0.9838 - val_loss: 0.3181 - val_fbeta_score: 0.9163\n",
      "Epoch 243/400\n",
      "774/774 [==============================] - 17s - loss: 0.0558 - fbeta_score: 0.9851 - val_loss: 0.3226 - val_fbeta_score: 0.9163\n",
      "Epoch 244/400\n",
      "774/774 [==============================] - 15s - loss: 0.0480 - fbeta_score: 0.9870 - val_loss: 0.3166 - val_fbeta_score: 0.9244\n",
      "Epoch 245/400\n",
      "774/774 [==============================] - 15s - loss: 0.0508 - fbeta_score: 0.9773 - val_loss: 0.3126 - val_fbeta_score: 0.9267\n",
      "Epoch 246/400\n",
      "774/774 [==============================] - 14s - loss: 0.0276 - fbeta_score: 0.9897 - val_loss: 0.3286 - val_fbeta_score: 0.9292\n",
      "Epoch 247/400\n",
      "774/774 [==============================] - 15s - loss: 0.0471 - fbeta_score: 0.9858 - val_loss: 0.3381 - val_fbeta_score: 0.9192\n",
      "Epoch 248/400\n",
      "774/774 [==============================] - 15s - loss: 0.0530 - fbeta_score: 0.9812 - val_loss: 0.3217 - val_fbeta_score: 0.9112\n",
      "Epoch 249/400\n",
      "774/774 [==============================] - 14s - loss: 0.0444 - fbeta_score: 0.9863 - val_loss: 0.3051 - val_fbeta_score: 0.9192\n",
      "Epoch 250/400\n",
      "774/774 [==============================] - 15s - loss: 0.0529 - fbeta_score: 0.9812 - val_loss: 0.2940 - val_fbeta_score: 0.9188\n",
      "Epoch 251/400\n",
      "774/774 [==============================] - 17s - loss: 0.0401 - fbeta_score: 0.9877 - val_loss: 0.2916 - val_fbeta_score: 0.9269\n",
      "Epoch 252/400\n",
      "774/774 [==============================] - 15s - loss: 0.0564 - fbeta_score: 0.9805 - val_loss: 0.2845 - val_fbeta_score: 0.9316\n",
      "Epoch 253/400\n",
      "774/774 [==============================] - 15s - loss: 0.0476 - fbeta_score: 0.9863 - val_loss: 0.2755 - val_fbeta_score: 0.9162\n",
      "Epoch 254/400\n",
      "774/774 [==============================] - 14s - loss: 0.0566 - fbeta_score: 0.9838 - val_loss: 0.2851 - val_fbeta_score: 0.9294\n",
      "Epoch 255/400\n",
      "774/774 [==============================] - 15s - loss: 0.0501 - fbeta_score: 0.9818 - val_loss: 0.2871 - val_fbeta_score: 0.9058\n",
      "Epoch 256/400\n",
      "774/774 [==============================] - 14s - loss: 0.0342 - fbeta_score: 0.9871 - val_loss: 0.3006 - val_fbeta_score: 0.9267\n",
      "Epoch 257/400\n",
      "774/774 [==============================] - 14s - loss: 0.0441 - fbeta_score: 0.9819 - val_loss: 0.3053 - val_fbeta_score: 0.9219\n",
      "Epoch 258/400\n",
      "774/774 [==============================] - 14s - loss: 0.0369 - fbeta_score: 0.9851 - val_loss: 0.3066 - val_fbeta_score: 0.9243\n",
      "Epoch 259/400\n",
      "774/774 [==============================] - 14s - loss: 0.0435 - fbeta_score: 0.9825 - val_loss: 0.3111 - val_fbeta_score: 0.9215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b1eb198>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(1, 50, 37), activation='relu'))\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid', activation='relu'))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['fbeta_score'])\n",
    "\n",
    "history = History()\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=40)\n",
    "\n",
    "# Split the train and test - we want to train on 80% of the data, and 20% for testing\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train, test in skf.split(xin, yin):\n",
    "    train = train\n",
    "    test = test\n",
    "    \n",
    "x_train = np.reshape(xin[train], (-1,1,50,37), order='C')\n",
    "x_test  = np.reshape(xin[test], (-1,1,50,37), order='C')\n",
    "y_test  = np.reshape(yin[test], (-1,1))\n",
    "y_train = np.reshape(yin[train], (-1,1))\n",
    "enc.fit(y_test)\n",
    "y_test  = enc.transform(y_test).toarray()\n",
    "enc.fit(y_train)\n",
    "y_train = enc.transform(y_train).toarray()\n",
    "model.fit(x_train, y_train, batch_size=64, nb_epoch=400, validation_data=(x_test, y_test), verbose=1, callbacks=[history, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "966/966 [==============================] - 11s - loss: 0.1827 - fbeta_score: 0.9460    \n",
      "Epoch 2/200\n",
      "966/966 [==============================] - 11s - loss: 0.1766 - fbeta_score: 0.9510    \n",
      "Epoch 3/200\n",
      "966/966 [==============================] - 11s - loss: 0.1381 - fbeta_score: 0.9566    \n",
      "Epoch 4/200\n",
      "966/966 [==============================] - 11s - loss: 0.1219 - fbeta_score: 0.9598    \n",
      "Epoch 5/200\n",
      "966/966 [==============================] - 11s - loss: 0.1265 - fbeta_score: 0.9624    \n",
      "Epoch 6/200\n",
      "966/966 [==============================] - 12s - loss: 0.1131 - fbeta_score: 0.9647    \n",
      "Epoch 7/200\n",
      "966/966 [==============================] - 11s - loss: 0.1147 - fbeta_score: 0.9630    \n",
      "Epoch 8/200\n",
      "966/966 [==============================] - 11s - loss: 0.0816 - fbeta_score: 0.9667    \n",
      "Epoch 9/200\n",
      "966/966 [==============================] - 11s - loss: 0.1119 - fbeta_score: 0.9589    \n",
      "Epoch 10/200\n",
      "966/966 [==============================] - 11s - loss: 0.1152 - fbeta_score: 0.9693    \n",
      "Epoch 11/200\n",
      "966/966 [==============================] - 11s - loss: 0.0966 - fbeta_score: 0.9688    \n",
      "Epoch 12/200\n",
      "966/966 [==============================] - 11s - loss: 0.0816 - fbeta_score: 0.9723    \n",
      "Epoch 13/200\n",
      "966/966 [==============================] - 11s - loss: 0.0870 - fbeta_score: 0.9740    \n",
      "Epoch 14/200\n",
      "966/966 [==============================] - 15s - loss: 0.0899 - fbeta_score: 0.9719    \n",
      "Epoch 15/200\n",
      "966/966 [==============================] - 12s - loss: 0.0590 - fbeta_score: 0.9787    \n",
      "Epoch 16/200\n",
      "966/966 [==============================] - 11s - loss: 0.0587 - fbeta_score: 0.9787    \n",
      "Epoch 17/200\n",
      "966/966 [==============================] - 11s - loss: 0.1025 - fbeta_score: 0.9688    \n",
      "Epoch 18/200\n",
      "966/966 [==============================] - 12s - loss: 0.0811 - fbeta_score: 0.9699    \n",
      "Epoch 19/200\n",
      "966/966 [==============================] - 11s - loss: 0.0958 - fbeta_score: 0.9693    \n",
      "Epoch 20/200\n",
      "966/966 [==============================] - 11s - loss: 0.0828 - fbeta_score: 0.9730    \n",
      "Epoch 21/200\n",
      "966/966 [==============================] - 11s - loss: 0.0621 - fbeta_score: 0.9802    \n",
      "Epoch 22/200\n",
      "966/966 [==============================] - 12s - loss: 0.0499 - fbeta_score: 0.9885    \n",
      "Epoch 23/200\n",
      "966/966 [==============================] - 11s - loss: 0.0687 - fbeta_score: 0.9777    \n",
      "Epoch 24/200\n",
      "966/966 [==============================] - 11s - loss: 0.0670 - fbeta_score: 0.9776    \n",
      "Epoch 25/200\n",
      "966/966 [==============================] - 11s - loss: 0.0569 - fbeta_score: 0.9802    \n",
      "Epoch 26/200\n",
      "966/966 [==============================] - 11s - loss: 0.0753 - fbeta_score: 0.9709    \n",
      "Epoch 27/200\n",
      "966/966 [==============================] - 11s - loss: 0.0456 - fbeta_score: 0.9870    \n",
      "Epoch 28/200\n",
      "966/966 [==============================] - 11s - loss: 0.0637 - fbeta_score: 0.9782    \n",
      "Epoch 29/200\n",
      "966/966 [==============================] - 11s - loss: 0.0396 - fbeta_score: 0.9886    \n",
      "Epoch 30/200\n",
      "966/966 [==============================] - 11s - loss: 0.0528 - fbeta_score: 0.9865    \n",
      "Epoch 31/200\n",
      "966/966 [==============================] - 11s - loss: 0.0597 - fbeta_score: 0.9782    \n",
      "Epoch 32/200\n",
      "966/966 [==============================] - 11s - loss: 0.0552 - fbeta_score: 0.9860    \n",
      "Epoch 33/200\n",
      "966/966 [==============================] - 11s - loss: 0.0420 - fbeta_score: 0.9854    \n",
      "Epoch 34/200\n",
      "966/966 [==============================] - 11s - loss: 0.0617 - fbeta_score: 0.9818    \n",
      "Epoch 35/200\n",
      "966/966 [==============================] - 11s - loss: 0.1057 - fbeta_score: 0.9698    \n",
      "Epoch 36/200\n",
      "966/966 [==============================] - 11s - loss: 0.0858 - fbeta_score: 0.9703    \n",
      "Epoch 37/200\n",
      "966/966 [==============================] - 11s - loss: 0.0456 - fbeta_score: 0.9870    \n",
      "Epoch 38/200\n",
      "966/966 [==============================] - 11s - loss: 0.0397 - fbeta_score: 0.9860    \n",
      "Epoch 39/200\n",
      "966/966 [==============================] - 11s - loss: 0.0561 - fbeta_score: 0.9787    \n",
      "Epoch 40/200\n",
      "966/966 [==============================] - 11s - loss: 0.0735 - fbeta_score: 0.9792    \n",
      "Epoch 41/200\n",
      "966/966 [==============================] - 11s - loss: 0.0654 - fbeta_score: 0.9834    \n",
      "Epoch 42/200\n",
      "966/966 [==============================] - 16s - loss: 0.0398 - fbeta_score: 0.9880    \n",
      "Epoch 43/200\n",
      "966/966 [==============================] - 15s - loss: 0.0464 - fbeta_score: 0.9850    \n",
      "Epoch 44/200\n",
      "966/966 [==============================] - 11s - loss: 0.0673 - fbeta_score: 0.9818    \n",
      "Epoch 45/200\n",
      "966/966 [==============================] - 12s - loss: 0.0675 - fbeta_score: 0.9782    \n",
      "Epoch 46/200\n",
      "966/966 [==============================] - 12s - loss: 0.0539 - fbeta_score: 0.9844    \n",
      "Epoch 47/200\n",
      "966/966 [==============================] - 11s - loss: 0.0454 - fbeta_score: 0.9876    \n",
      "Epoch 48/200\n",
      "966/966 [==============================] - 11s - loss: 0.0437 - fbeta_score: 0.9870    \n",
      "Epoch 49/200\n",
      "966/966 [==============================] - 12s - loss: 0.0535 - fbeta_score: 0.9782    \n",
      "Epoch 50/200\n",
      "966/966 [==============================] - 12s - loss: 0.0633 - fbeta_score: 0.9802    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1177f1898>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_xin   = np.reshape(xin, (-1,1,50,37), order='C')\n",
    "img_xout  = np.reshape(xout, (-1,1,50,37), order='C')\n",
    "yin   = np.reshape(yin, (-1,1))\n",
    "enc.fit(yin)\n",
    "yin   = enc.transform(yin).toarray()\n",
    "history_all = History()\n",
    "earlystopping_all = EarlyStopping(monitor='loss', min_delta=0, patience=20)\n",
    "model.fit(img_xin, yin, nb_epoch=200, batch_size=64, callbacks=[history_all, earlystopping_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1, 50, 37)\n",
      "35/35 [==============================] - 0s\n",
      "(35, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load heldout testing dataset\n",
    "xout = np.load('./corrected.lfw.heldout.npy')\n",
    "img_xout = np.reshape(xout, (-1,1,50,37), order='C')\n",
    "print(img_xout.shape)\n",
    "y_pred = model.predict(img_xout, batch_size=64, verbose=1)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322, 1, 50, 37)\n",
      "322/322 [==============================] - 1s     \n",
      "(322, 7)\n"
     ]
    }
   ],
   "source": [
    "img_xout = np.reshape(xout, (322,1,50,37), order='C')\n",
    "print(img_xout.shape)\n",
    "y_pred = model.predict(img_xout, batch_size=64, verbose=1)\n",
    "print(y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert from one-hot encoding to integer labels\n",
    "y_predicted = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model for reproductibility purposes\n",
    "model.save('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createSubmission(\"submission_heldout_testing.csv\", y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of Individual Work\n",
    "\n",
    "Please initial (between the square brackets) one of the following statements.\n",
    "\n",
    "I, [FP] A0153447A, [TSK] A0097689Y, [MG] A0153196B  , certify that I have followed the CS 3244 Machine Learning class guidelines for homework assignments.  In particular, I expressly vow that I have followed the Facebook rule in discussing with others in doing the assignment and did not take notes (digital or printed) from the discussions.  \n",
    "\n",
    "I suggest that I should be graded as follows:\n",
    "\n",
    "Effort + Leaderboard! \n",
    "\n",
    "### References\n",
    "\n",
    "I have refered to the following list of people and websites in preparing my homework submission:\n",
    "\n",
    "http://docs.scipy.org/doc/numpy/reference/,\n",
    "http://stackoverflow.com/,\n",
    "http://matplotlib.org/api/pyplot_summary.html,\n",
    "Textbook: Learning From Data,\n",
    "Lecture's slides\n",
    "https://keras.io/ - Keras Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:hw3]",
   "language": "python",
   "name": "conda-env-hw3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
